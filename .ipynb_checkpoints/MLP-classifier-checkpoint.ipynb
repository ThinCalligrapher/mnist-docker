{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b746e3f-d120-4136-a2fb-52384721e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd21763d-f98a-4773-90ce-7a33579fc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dc1216-38d4-4e06-99c3-11604cb016fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "data = datasets.MNIST(path, download=True)\n",
    "# if you run this locally you will download 70,000 images!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9f46b7-dba8-4caf-8203-373871ae0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5,), (.5,))\n",
    "])\n",
    "\n",
    "# transforms our data with transform(x)\n",
    "# ToTensor takes our data(in this case pixel values from 0 - 255) and converts it to a Tensor with values from 0 - 1.0\n",
    "# Normalize takes 2 tuples, and does (x - t1) / t2 then passes the data\n",
    "# we get this from torchvision, specifically the torchvision.transforms.Compose library, which will take our data and sequentially transfrom it to fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805aa61e-3f18-4200-acd7-b662e6ff8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078146c-5136-42fb-815f-548bc03847b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea50eb6-7652-4c4f-be47-4031ddf27ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "plt.imshow(images[0].squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"Label: {labels[0].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eedb7d3-a47c-4483-a70d-86e00b325325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now on to the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e58b25-8296-4bf3-acd8-bb8ddc6a0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),   # takes the 28x28 image into [784]\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(), # first non linear layer\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), # second non linear layer\n",
    "            nn.Linear(64, 10)    # output of 10 classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2bb53cd-a24a-41b2-b1f3-2a5a59c5d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DigitClassifier(\n",
      "  (net): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# now instantiate the model\n",
    "model = DigitClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f84828-d558-464c-ba9c-75b43734d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define loss function and hyperparameters\n",
    "# due to our output being a vector or scores, we need to use CEL\n",
    "# this will turn out outputs into a probability distribution\n",
    "# via Softmax\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37aad0c9-5c6b-491e-883b-d8d707baf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Adam due to its dynamic learning rate, and it\n",
    "# will keep us out of local minimums due to a term in the denominator of the gradient equation (look more into this)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bd83b9-97c2-4f81-b3d6-07905db83f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e804c34b-c34b-4b4b-9427-7da20251ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0853\n",
      "Test Accuracy: 96.85%\n",
      "Epoch 2, Loss: 0.0741\n",
      "Test Accuracy: 97.14%\n",
      "Epoch 3, Loss: 0.0668\n",
      "Test Accuracy: 97.03%\n",
      "Epoch 4, Loss: 0.0618\n",
      "Test Accuracy: 97.18%\n",
      "Epoch 5, Loss: 0.0540\n",
      "Test Accuracy: 97.46%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train() # place model in training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # forward pass\n",
    "        outputs = model(images) # shape of [64, 10] due to data being fed via the loader\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backwards and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # evaluation\n",
    "    model.eval() # set to eval mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # no gradients for testing\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1) # pick highest score class\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483a448-209e-4167-b96d-575c641c2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation of the architecture\n",
    "# \n",
    "#    \n",
    "#    \n",
    "#        \n",
    "#        \n",
    "#    \n",
    "#        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae344d00-c428-4738-9c78-b01ad74569fa",
   "metadata": {},
   "source": [
    "# Explanation of the architecture\n",
    "---\n",
    "### This is a feedforward multi-layer perceptron (MLP) neural network\n",
    "1. we start by flattening the data (turning a 2D image into a line)\n",
    "2. the first layer is a linear layer, where we take the 784 data points and connect them to 128 neurons, each with their own weight and bias\n",
    "    - this has 784*128 + 128 parameters (weights and biases)\n",
    "3. The ReLU is the first non linear layer, which lets our network learn complex decision boundaries\n",
    "4. The second linear layer goes from 128 to 64 units, giving us more depth and ability to detect features\n",
    "    - this layer has 128 * 64 + 64 parameters\n",
    "5. The second ReLU, for more nonlinearity\n",
    "6. The Output layer, taking us from 64 units to 10. Each of these 10 units are a \"raw\" score for our numbers 0-9\n",
    "    - Softmax takes these and assigns a probability inside of our loss function (CLE)\n",
    "  \n",
    "- The first layer learns low level patterns (strokes, edges)\n",
    "- The second Linear layer learns higher level ideas like loops, shapes, and corners\n",
    "\n",
    "### In the backpropegation and optimization, each one of these parameters is differentiated and its gradient is found. Adam will change our parameter learning rates depending on momentum (how fast we are approaching \"correct\").\n",
    "\n",
    "## Why do we have train_loader and test_loader?\n",
    "- This is because MNIST is a dataset of 70,000 images. We cannot pass all of the images at once, so we need to feed them into our model slowly. Our batch size in this instance was 64 images.\n",
    "- We have a training data set of 60,000 images, and our test dataset is 10,000. These must be kept separate so we do not train our model with images that we test its accuracy.\n",
    "\n",
    "\n",
    "# How to take this further\n",
    "- Create a classifier to classify images in the real world (background challenges, noise)\n",
    "- Use a convolutional neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad947d-54eb-4df6-b014-40063dfcc844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyterenv)",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
