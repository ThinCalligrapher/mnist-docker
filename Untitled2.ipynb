{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fabdd19-48f6-49aa-b6c2-c5ce0c35245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make some synthetic data, and do a linear model with the steps plainly laid out\n",
    "# its important to be able to push myself to engage with the content and remember\n",
    "# that is what's going to be my superpower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b91a0f7-6be9-4182-ae89-c609772cc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59bdfaa3-8efd-49c6-a169-b146e5e5f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "t2 = t1 * 3 + 7 + torch.randn(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47c0a5c2-16f1-42fd-81a9-c7e0c2a486a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for every machine learning program you need a loss function, an optimizer, yep ohhh and the model as well which you can get from nn\n",
    "model = torch.nn.Linear(1, 1)\n",
    "loss_fn = nn.MSELoss()\n",
    "# the basic loss function would have to be made inside of the training loop\n",
    "# fuck I seem to have forgotten how to do this\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.001)\n",
    "epochs = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "196da5a7-3641-484a-82ec-fba730796136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "torch.optim.SGD(\n",
       "    params: Union[collections.abc.Iterable[torch.Tensor], collections.abc.Iterable[dict[str, Any]], collections.abc.Iterable[tuple[str, torch.Tensor]]],\n",
       "    lr: Union[float, torch.Tensor] = \u001b[32m0.001\u001b[39m,\n",
       "    momentum: float = \u001b[32m0\u001b[39m,\n",
       "    dampening: float = \u001b[32m0\u001b[39m,\n",
       "    weight_decay: Union[float, torch.Tensor] = \u001b[32m0\u001b[39m,\n",
       "    nesterov: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    *,\n",
       "    maximize: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    foreach: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    differentiable: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    fused: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Implements stochastic gradient descent (optionally with momentum).\n",
       "\n",
       ".. math::\n",
       "   \\begin{aligned}\n",
       "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
       "        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
       "            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
       "        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
       "        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
       "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
       "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
       "        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n",
       "        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n",
       "        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
       "        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
       "        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
       "        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
       "        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
       "        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
       "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
       "        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
       "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
       "        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
       "        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                               \\\\\n",
       "        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
       "        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
       "        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                    \\\\[-1.ex]\n",
       "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
       "        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
       "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
       "   \\end{aligned}\n",
       "\n",
       "Nesterov momentum is based on the formula from\n",
       "`On the importance of initialization and momentum in deep learning`__.\n",
       "\n",
       "Args:\n",
       "    params (iterable): iterable of parameters or named_parameters to optimize\n",
       "        or iterable of dicts defining parameter groups. When using named_parameters,\n",
       "        all parameters in all groups should be named\n",
       "    lr (float, Tensor, optional): learning rate (default: 1e-3)\n",
       "    momentum (float, optional): momentum factor (default: 0)\n",
       "    dampening (float, optional): dampening for momentum (default: 0)\n",
       "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
       "    nesterov (bool, optional): enables Nesterov momentum. Only applicable\n",
       "        when momentum is non-zero. (default: False)\n",
       "    maximize (bool, optional): maximize the objective with respect to the\n",
       "        params, instead of minimizing (default: False)\n",
       "    foreach (bool, optional): whether foreach implementation of optimizer\n",
       "        is used. If unspecified by the user (so foreach is None), we will try to use\n",
       "        foreach over the for-loop implementation on CUDA, since it is usually\n",
       "        significantly more performant. Note that the foreach implementation uses\n",
       "        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n",
       "        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n",
       "        parameters through the optimizer at a time or switch this flag to False (default: None)\n",
       "    differentiable (bool, optional): whether autograd should\n",
       "        occur through the optimizer step in training. Otherwise, the step()\n",
       "        function runs in a torch.no_grad() context. Setting to True can impair\n",
       "        performance, so leave it False if you don't intend to run autograd\n",
       "        through this instance (default: False)\n",
       "    fused (bool, optional): whether the fused implementation is used.\n",
       "        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n",
       "        are supported. (default: None)\n",
       "\n",
       ".. note:: The foreach and fused implementations are typically faster than the for-loop,\n",
       "          single-tensor implementation, with fused being theoretically fastest with both\n",
       "          vertical and horizontal fusion. As such, if the user has not specified either\n",
       "          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n",
       "          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n",
       "          implementation is relatively new, we want to give it sufficient bake-in time.\n",
       "          To specify fused, pass True for fused. To force running the for-loop\n",
       "          implementation, pass False for either foreach or fused. \n",
       "\n",
       "\n",
       "Example:\n",
       "    >>> # xdoctest: +SKIP\n",
       "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
       "    >>> optimizer.zero_grad()\n",
       "    >>> loss_fn(model(input), target).backward()\n",
       "    >>> optimizer.step()\n",
       "\n",
       "__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
       "\n",
       ".. note::\n",
       "    The implementation of SGD with Momentum/Nesterov subtly differs from\n",
       "    Sutskever et al. and implementations in some other frameworks.\n",
       "\n",
       "    Considering the specific case of Momentum, the update can be written as\n",
       "\n",
       "    .. math::\n",
       "        \\begin{aligned}\n",
       "            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
       "            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
       "        \\end{aligned}\n",
       "\n",
       "    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
       "    parameters, gradient, velocity, and momentum respectively.\n",
       "\n",
       "    This is in contrast to Sutskever et al. and\n",
       "    other frameworks which employ an update of the form\n",
       "\n",
       "    .. math::\n",
       "        \\begin{aligned}\n",
       "            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
       "            p_{t+1} & = p_{t} - v_{t+1}.\n",
       "        \\end{aligned}\n",
       "\n",
       "    The Nesterov version is analogously modified.\n",
       "\n",
       "    Moreover, the initial value of the momentum buffer is set to the\n",
       "    gradient value at the first step. This is in contrast to some other\n",
       "    frameworks that initialize it to all zeros. One notable side effect\n",
       "    of this decision is that the first momentum value will not be scaled\n",
       "    by dampening. Dampening will be applied starting at the second step.\n",
       "\u001b[31mFile:\u001b[39m           ~/jupyterenv/lib/python3.12/site-packages/torch/optim/sgd.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.optim.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7c19436-8823-48be-a2fc-c6000f2a1cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "torch.nn.Linear(\n",
       "    in_features: int,\n",
       "    out_features: int,\n",
       "    bias: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    device=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dtype=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
       "\n",
       "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
       "\n",
       "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
       "\n",
       "Args:\n",
       "    in_features: size of each input sample\n",
       "    out_features: size of each output sample\n",
       "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
       "        Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*, H_\\text{in})` where :math:`*` means any number of\n",
       "      dimensions including none and :math:`H_\\text{in} = \\text{in\\_features}`.\n",
       "    - Output: :math:`(*, H_\\text{out})` where all but the last dimension\n",
       "      are the same shape as the input and :math:`H_\\text{out} = \\text{out\\_features}`.\n",
       "\n",
       "Attributes:\n",
       "    weight: the learnable weights of the module of shape\n",
       "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
       "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
       "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
       "            If :attr:`bias` is ``True``, the values are initialized from\n",
       "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Linear(20, 30)\n",
       "    >>> input = torch.randn(128, 20)\n",
       "    >>> output = m(input)\n",
       "    >>> print(output.size())\n",
       "    torch.Size([128, 30])\n",
       "\u001b[31mInit docstring:\u001b[39m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[31mFile:\u001b[39m           ~/jupyterenv/lib/python3.12/site-packages/torch/nn/modules/linear.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80de2e15-d2cb-46cf-9ee0-646179d9658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: 3.260939598083496\n",
      "bias: 5.521503925323486\n",
      "weight: 3.25262451171875\n",
      "bias: 5.545950889587402\n",
      "weight: 3.2445549964904785\n",
      "bias: 5.569676876068115\n",
      "weight: 3.2367236614227295\n",
      "bias: 5.592701435089111\n",
      "weight: 3.229123592376709\n",
      "bias: 5.615047454833984\n",
      "weight: 3.221747398376465\n",
      "bias: 5.636734962463379\n",
      "weight: 3.2145886421203613\n",
      "bias: 5.657782077789307\n",
      "weight: 3.207641124725342\n",
      "bias: 5.678208351135254\n",
      "weight: 3.2008984088897705\n",
      "bias: 5.698033332824707\n",
      "weight: 3.194354772567749\n",
      "bias: 5.7172722816467285\n",
      "weight: 3.1880040168762207\n",
      "bias: 5.73594331741333\n",
      "weight: 3.181840658187866\n",
      "bias: 5.754064083099365\n",
      "weight: 3.175859212875366\n",
      "bias: 5.771651268005371\n",
      "weight: 3.1700541973114014\n",
      "bias: 5.7887187004089355\n",
      "weight: 3.1644203662872314\n",
      "bias: 5.805282115936279\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    pred = model(t1)\n",
    "    loss = loss_fn(pred, t2)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"weight: {model.weight.item()}\")\n",
    "        print(f\"bias: {model.bias.item()}\")\n",
    "\n",
    "    # ok so in general I need to make my prediction, get the loss, use that loss to backpropegate.then optimize then zero the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d039d103-556d-4be5-a384-99218b62a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = torch.tensor([[5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf26cd5d-5699-4eee-97e0-03a314eb3f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21.6162]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7857a49a-1e46-459a-95d2-86f9bd8b6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = \"help\"\n",
    "r = [c for c in t]\n",
    "y = r[:]\n",
    "y.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "725e508b-7b63-44c5-bedf-0df099a31325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listed = [1, 2, 3, 4, 5]\n",
    "sum(listed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "029352aa-6141-4a36-8a6c-f3876b059985",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in range(1, 1):\n",
    "    print(\"yeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fae0ba99-f618-4ffc-9b9d-81d427db31c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d836f34-e766-405a-bd2c-0c67339fbbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyterenv)",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
